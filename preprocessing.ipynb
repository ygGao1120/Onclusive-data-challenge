{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFN65o+SZHbFJbpiqEb2FB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ygGao1120/Onclusive-data-challenge/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo-RoNxronTv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('drive/MyDrive/interview/Onclusive data assignment')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "VC_UDSclqN9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from operator import itemgetter\n",
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "iQ0mdr9GqPuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "train = pd.read_csv('train.tsv',sep = '\\t')\n",
        "valid = pd.read_csv('dev.tsv',sep = '\\t')\n",
        "test = pd.read_csv('test.tsv',sep = '\\t')\n"
      ],
      "metadata": {
        "id": "oVYO413kqU6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop missin value in the 4 columns\n",
        "train = train.dropna(axis=0,subset = ['label','claim','explanation','main_text'])\n",
        "valid = valid.dropna(axis=0,subset = ['label','claim','explanation','main_text'])\n",
        "test = test.dropna(axis=0,subset = ['label','claim','explanation','main_text'])"
      ],
      "metadata": {
        "id": "o3UHLwIyqaC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer labels into numbers\n",
        "train['label'].replace(to_replace = ['false','mixture','true','unproven','snopes'],value = [0,1,2,3,4],inplace = True)\n",
        "valid['label'].replace(to_replace = ['false','mixture','true','unproven','National, Candidate Biography, Donald Trump, '],value = [0,1,2,3,5],inplace = True)\n",
        "test['label'].replace(to_replace = ['false','mixture','true','unproven'],value = [0,1,2,3],inplace = True)"
      ],
      "metadata": {
        "id": "JucwxOXpqt9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['label'] = train['label'].astype(int)\n",
        "valid['label'] = valid['label'].astype(int)\n",
        "test['label'] = test['label'].astype(int)"
      ],
      "metadata": {
        "id": "tkJrj-Zmqybl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for selecting the sentences most related to claim in main_text\n",
        "def select_evidence_sentences(corpus, k = 5):\n",
        "    \"\"\"Select top k evidence sentences (in main_text) based on sentence transformer model.\"\"\"\n",
        "    sentence_transformer_model = SentenceTransformer('bert-base-nli-mean-tokens')#sentence bert\n",
        "    corpus['top_k'] = np.empty([len(corpus),], dtype=str)\n",
        "\n",
        "    for index, row in corpus.iterrows():\n",
        "        claim = row['claim']\n",
        "        sentences = [claim] + [\n",
        "                     sentence for sentence in sent_tokenize(row['main_text'])]\n",
        "\n",
        "        sentence_embeddings = sentence_transformer_model.encode(sentences)# output is matrix\n",
        "        #print('sentence_embeddings',sentence_embeddings)\n",
        "        claim_embedding = sentence_embeddings[0]# first row of sentence_embeddings\n",
        "        #print('claim_embeddings',claim_embedding)\n",
        "        sentence_embeddings = sentence_embeddings[1:]\n",
        "        cosine_similarity_emb = {}\n",
        "\n",
        "        for sentence, embedding in zip(sentences, sentence_embeddings):\n",
        "            cosine_similarity_emb[sentence] = np.linalg.norm(cosine_similarity(\n",
        "                [claim_embedding, embedding]))\n",
        "\n",
        "        top_k = dict(sorted(cosine_similarity_emb.items(), \n",
        "                            key=itemgetter(1))[:k])\n",
        "        if not top_k:\n",
        "            top_k = row['main_text']\n",
        "        else:\n",
        "            corpus.at[index, 'top_k'] = ' '.join(key for key in top_k.keys())\n",
        "\n",
        "    df = pd.DataFrame(columns=['claim', 'top_k', 'label', 'explanation'])\n",
        "    df['claim'] = corpus['claim']\n",
        "    df['top_k'] = corpus['top_k']\n",
        "    df['label'] = corpus['label']\n",
        "    df['explanation'] = corpus['explanation']\n",
        "    return df"
      ],
      "metadata": {
        "id": "cmICvitHq0x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train = select_evidence_sentences(train, k = 5)\n",
        "Valid = select_evidence_sentences(valid, k = 5)\n",
        "Test = select_evidence_sentences(test, k = 5)"
      ],
      "metadata": {
        "id": "zgnvppxXq9xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train.top_k.replace('',np.nan,inplace = True)\n",
        "Train.top_k.replace(0,np.nan,inplace = True)\n",
        "Train.explanation.replace(' ',np.nan,inplace = True)"
      ],
      "metadata": {
        "id": "CIFwsh7krFTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train = Train.dropna(axis=0,subset = ['top_k', 'explanation'])# a nan in top_k of Train\n",
        "Valid = Valid.dropna(axis=0,subset = ['top_k', 'explanation'])\n",
        "Test = Test.dropna(axis=0,subset = ['top_k', 'explanation'])"
      ],
      "metadata": {
        "id": "lPChl-EetDCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare for sentence cleaning process\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_word = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "ojC275t8tDpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for data cleaning\n",
        "def clean_str(s):\n",
        "    #remove symbols and numbers\n",
        "    s = re.sub(r\"\\\\n\",\"\",s)\n",
        "    s = re.sub(r\"[^A-Za-z]\",\" \",s)\n",
        "    s = s.strip().lower()\n",
        "    \n",
        "    #tokenize\n",
        "    s = word_tokenize(s)\n",
        "    \n",
        "    #Stemming\n",
        "    c = []\n",
        "    for word in s:\n",
        "      doc = nlp(word)\n",
        "      for token in doc:\n",
        "        #remove stop word\n",
        "        if token.lemma_ not in stop_word:\n",
        "            c.append(token.lemma_)\n",
        "    \n",
        "    return c"
      ],
      "metadata": {
        "id": "Jd-KnK5UtHr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning\n",
        "Train['clean_claim'] = Train.apply(lambda x: clean_str(x.claim),axis = 1)\n",
        "Train['clean_top_k'] = Train.apply(lambda x: clean_str(x.top_k),axis = 1)\n",
        "Train['clean_explanation'] = Train.apply(lambda x: clean_str(x.explanation),axis = 1)\n",
        "\n",
        "Valid['clean_claim'] = Valid.apply(lambda x: clean_str(x.claim),axis = 1)\n",
        "Valid['clean_top_k'] = Valid.apply(lambda x: clean_str(x.top_k),axis = 1)\n",
        "Valid['clean_explanation'] = Valid.apply(lambda x: clean_str(x.explanation),axis = 1)\n",
        "\n",
        "Test['clean_claim'] = Test.apply(lambda x: clean_str(x.claim),axis = 1)\n",
        "Test['clean_top_k'] = Test.apply(lambda x: clean_str(x.top_k),axis = 1)\n",
        "Test['clean_explanation'] = Test.apply(lambda x: clean_str(x.explanation),axis = 1)"
      ],
      "metadata": {
        "id": "Wm8-D-9vtKc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train = Train.drop(columns = 'Unnamed: 0')\n",
        "Valid = Valid.drop(columns = 'Unnamed: 0')\n",
        "Test = Test.drop(columns = 'Unnamed: 0')"
      ],
      "metadata": {
        "id": "dLomRq70tW1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train = Train.loc[Train.label.isin([0,1,2,3])]\n",
        "Valid = Valid.loc[Valid.label.isin([0,1,2,3])]\n",
        "Test = Test.loc[Test.label.isin([0,1,2,3])]"
      ],
      "metadata": {
        "id": "R3Ful18Xtak7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saved as excel\n",
        "Train.to_excel('cleaned_train_list.xlsx')\n",
        "Valid.to_excel('cleaned_valid_list.xlsx')\n",
        "Test.to_excel('cleaned_test_list.xlsx')"
      ],
      "metadata": {
        "id": "R_DdpuDltltJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}